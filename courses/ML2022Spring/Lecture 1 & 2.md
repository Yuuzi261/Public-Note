# Lecture 1 & Lecture 2

## æ©Ÿå™¨å­¸ç¿’åŸºæœ¬æ¦‚å¿µç°¡ä»‹

### ä½•è¬‚æ©Ÿå™¨å­¸ç¿’

**Q: ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?**</br>
*A: æ¦‚æ‹¬ä¾†èªªï¼Œè®“æ©Ÿå™¨å…·å‚™æ‰¾ä¸€å€‹å‡½å¼çš„èƒ½åŠ›!*
![](https://i.imgur.com/xNtqsVZ.png)

### å‡½æ•¸çš„é¡å‹

ä¸Šé¢æåˆ°æ©Ÿå™¨è¦æ‰¾ä¸€å€‹å‡½æ•¸ï¼Œé€™å€‹å‡½æ•¸è¢«å¤§è‡´åˆ†ç‚ºå¹¾ç¨®

* **Regression:** è¼¸å‡ºç‚ºä¸€å€‹æ•¸å€¼ </br>
![](https://i.imgur.com/cxaTsKh.png)
* **Classification:** å¾æ—¢æœ‰çš„é¸é … *(classes/é¡åˆ¥)* ä¸­ï¼Œé¸æ“‡å…¶ä¸€è¼¸å‡º
![](https://i.imgur.com/CKs1af4.png)
![](https://i.imgur.com/vXEwi7c.png)
* **Structured Learning:** è®“æ©Ÿå™¨ç”¢ç”Ÿä¸€å€‹æœ‰çµæ§‹çš„ç‰©ä»¶ *(e.g. ç•«ä¸€å¼µåœ–ã€å¯«ä¸€ç¯‡æ–‡ç« ...)* ï¼Œè®“æ©Ÿå™¨å­¸æœƒå‰µé€ 

### å¦‚ä½•æ‰¾ä¸€å€‹å‡½æ•¸

**ä»¥ç°¡å–®é æ¸¬YouTubeæµé‡ç‚ºä¾‹...**

#### 1. **Find with Unknown Parameters** å¯«å‡ºå¸¶æœ‰æœªçŸ¥åƒæ•¸çš„å‡½å¼

![](https://i.imgur.com/jUCKe8g.png)

#### 2. **Define Loss form Training Data** å®šç¾©æå¤±å‡½æ•¸

    * Loss is a function of parameters: L(b, w)
    * Loss: how good a set of value is

![](https://i.imgur.com/LyQPab9.png)

Loss: $L = \frac{1}{N}\Sigma e_n$

å°‡æ¯ä¸€å¤©çš„èª¤å·®å€¼åŠ ç¸½èµ·ä¾†å–å¹³å‡ï¼Œå°±èƒ½å¾—åˆ°æå¤±å‡½æ•¸çš„å€¼</br>
è¨ˆç®—èª¤å·®å€¼çš„æ–¹å¼å¤§è‡´ä¸Šæœ‰ä¸‹åˆ—2ç¨®æ–¹æ³•:

* **MAE** Mean Absolute Error
    $e = |y - \hat{y}|$
* **MSE** Mean Square Error
    $e = (y - \hat{y})^2$

ä¾ç…§éœ€æ±‚å’Œå°ä»»å‹™çš„ç†è§£ä½œé¸æ“‡</br>
*å¦‚æœ* $y$ *å’Œ* $\hat{y}$ *éƒ½æ˜¯æ©Ÿç‡åˆ†ä½ˆçš„è©±ï¼Œå‰‡æœƒé¸æ“‡Cross-entropy(ä¹‹å¾Œå†èªª)*

**Error Surface:**
å°‡å„ç¨®åƒæ•¸çµ„åˆå‡ºä¾†çš„Losså€¼ç¹ªè£½æˆç­‰é«˜ç·šåœ–
![](https://i.imgur.com/OWUXEpy.png)

#### 3. **Optimization** æœ€ä½³åŒ–

$$w^*, b^* = \arg\min\limits_{w, b}L$$
*æ‰¾ä¸€å€‹æœ€å¥½çš„* $w$*å’Œ* $b$ *(* $w^*$ & $b^*$ *)ï¼Œä½¿å¾—Lossçš„å€¼æœ€å°*
**Gradient Descent:**
å…ˆé®ä½å¦ä¸€å€‹åƒæ•¸ï¼Œæˆ‘å€‘å…ˆçœ‹å–®ä¸€çš„åƒæ•¸...
![](https://i.imgur.com/HKcwjqt.png)
:::info
$\eta$ : learning rate *(a hyperparameters è‡ªå·±è¨­å®šçš„åƒæ•¸)*</br>
larning rateè¶Šé«˜ï¼Œå­¸ç¿’è¶Šå¿« *(æ•¸å€¼è®ŠåŒ–å¿«)*
:::

å¦‚åœ–ï¼ŒGradient Descenté¡¯è€Œæ˜“è¦‹çš„å•é¡Œå³æ˜¯local minimaå•é¡Œï¼Œé€šå¸¸ç„¡æ³•æ‰¾åˆ°golbal minimaï¼Œä½†è€å¸«æåˆ°local minimaå…¶å¯¦æ˜¯å€‹å‡è­°é¡Œï¼ŒåšGradient Descentæ™‚æœƒé‡åˆ°çš„çœŸæ­£é›£é¡Œä¸¦ä¸æ˜¯local minimaå•é¡Œ *(ä¹‹å¾Œå†æGradient DescentçœŸæ­£çš„ç—›é»)*</br>
å–®ä¸€çš„åƒæ•¸ç†è§£ä¹‹å¾Œï¼Œå¤šå€‹åƒæ•¸ä¹Ÿæ˜¯ç›¸åŒæ¦‚å¿µ...

* (Randomly) Pick initial values $w^0, w^b$
* Compute
$$
\begin{align*}
w^1&\leftarrow w^0-\eta\frac{\delta L}{\delta w}|_{w = w^0, b = b^0} \\
b^1&\leftarrow b^0-\eta\frac{\delta L}{\delta b}|_{w = w^0, b = b^0}
\end{align*}
$$
* Update $w$ and $b$ interatively

### è¨“ç·´ & é æ¸¬

ä¸Šé¢çš„æ­¥é©Ÿå…¶å¯¦å°±æ˜¯åœ¨è¨“ç·´
![](https://i.imgur.com/Nt67jPa.png)
å¦‚åœ–ï¼Œåœ¨å·²çŸ¥çš„è³‡æ–™ä¸Šçš„æœ€å°Lossæ˜¯0.48kï¼Œè€Œåœ¨é æ¸¬çš„æ•¸æ“šä¸Šï¼ŒLosså‰‡æ˜¯ä¾†åˆ°0.58k </br></br>
![](https://i.imgur.com/4D6qlbk.png)
![](https://i.imgur.com/w1NUdrS.png)</br>
æ ¹æ“šç¬¬ä¸€æ¬¡é æ¸¬çš„çµæœï¼Œæˆ‘å€‘å¯ä»¥ç™¼ç¾é€™äº›è³‡æ–™æœ‰å­˜åœ¨é€±æœŸæ€§ *(åœ¨æ­¤ç‚º7å¤©ä¸€å¾ªç’°ï¼Œæ˜ŸæœŸå››å’Œæ˜ŸæœŸäº”çš„è§€çœ‹äººæ•¸éƒ½æœƒæ¸›å°‘)*ï¼Œåˆ©ç”¨é€™å€‹é€±æœŸæ€§ä¾†å˜—è©¦ä¿®æ”¹æ¨¡å‹...</br>
ä»¥ä¸Šé€™äº›`feature * weight + bias`çš„æ¨¡å‹å°±ç¨±ç‚º **Linear Models**

### æ¨¡å‹

Linear Modelså¾ˆé¡¯ç„¶æ˜¯ä¸å¤ çš„ï¼Œå®ƒæœ‰è‘—å¾ˆå¤§çš„é™åˆ¶ï¼Œç¨±ä½œ**Model Bias**ï¼Œä½¿å¾—é€™å€‹æ¨¡å‹ç„¡æ³•æ¨¡æ“¬çœŸå¯¦çš„ç‹€æ³ï¼Œå› æ­¤æˆ‘å€‘éœ€è¦æ›´è¤‡é›œã€æ›´æœ‰å½ˆæ€§çš„æ¨¡å‹
![](https://i.imgur.com/VMYl3Pd.png)

**Piecewise Linear Curves**
ä¸Šé¢é€™æ¢ç´…ç·šå…¶å¯¦å°±æ˜¯Piecewise Linear Curves *(åˆ†æ®µç·šæ€§æ›²ç·š)*ï¼Œæˆ‘å€‘å¯ä»¥ç™¼ç¾é€™ç¨®æ›²ç·šå¯ä»¥æ•´ç†æˆä¸‹åˆ—çš„å¼å­:</br>
<font color = "red">red curve</font> = constant + <font color = blue>sum of a set of blue cruve</font>

![](https://i.imgur.com/C3CjdKW.png)

å¦‚åœ–ï¼Œåˆ©ç”¨ä¸åŒçš„<font color = blue>è—è‰²æ›²ç·š</font>åŠ ä¸Šä¸€å€‹å¸¸æ•¸ï¼Œå°±å½¢æˆäº†<font color = red>ç´…è‰²åˆ†æ®µç·šæ€§æ›²ç·š</font> *(<font color = red>red curve</font> = 0 + 1 + 2 + 3 curves)*

**Q: ä½†xå’Œyçš„é—œä¿‚ä¸ä¸€å®šæ˜¯Piecewise Linear Curveså•Šï¼Œé‚£è©²æ€éº¼è¾¦?**</br>
*A: å…ˆåœ¨æ›²ç·šä¸Šå–å¹¾å€‹é»ï¼Œå†é€£èµ·ä¾†å½¢æˆPiecewise Linear Curvesï¼Œåªè¦é»å–å¾—å¤ å¥½æˆ–**å¤ å¤š**ï¼Œå°±èƒ½å’ŒåŸæœ¬çš„æ›²ç·šéå¸¸æ¥è¿‘*
![](https://i.imgur.com/rdsLvdH.png) </br></br>
**Q: é‚£è¦å¦‚ä½•å¾—åˆ°<font color = blue>è—è‰²æ›²ç·š</font>å‘¢?**</br>
*A: æˆ‘å€‘å¯ä»¥ç”¨**Sigmoid Function**ä¾†å˜—è©¦é€¼è¿‘å®ƒ*
![](https://i.imgur.com/HJf0UyJ.png) </br></br>
è€Œæˆ‘å€‘ä¸Šé¢ä¸€ç›´åœ¨è¬›çš„<font color = blue>è—è‰²æ›²ç·š</font>å‰‡æ˜¯å«åš**Hard Sigmoid**
![](https://i.imgur.com/r73qiKr.png) </br></br>
é€éæ”¹è®Š<font color = blue>$w$</font>ã€<font color = green>$b$</font>å’Œ<font color = red>$c$</font>ï¼Œå°±èƒ½å»é€¼è¿‘å‡ºå„ç¨®ä¸åŒçš„Sigmoid Function
![](https://i.imgur.com/x87SuGf.png)
![](https://i.imgur.com/Y8ccLGp.png)
![](https://i.imgur.com/t1BuUC8.png) </br></br>
å›åˆ°é€™ä¸€å¼µåœ–...
![](https://i.imgur.com/VfIYzfK.png)
æˆ‘å€‘å°±æˆåŠŸæŠŠ<font color = red>ç´…è‰²åˆ†æ®µæ›²ç·š</font>è¡¨ç¤ºå‡ºä¾†äº†</br>

**æ•´ç†ä¸€ä¸‹æˆ‘å€‘çš„æ–°æ¨¡å‹:**

* å–®å€‹feature
$$y = b + wx_1\\\downarrow\\ y = b + \sum_i \color{red}{c_i}sigmoid(\color{green}{b_i} + \color{blue}{w_i}x_1)$$
* å¤šå€‹feature

$$y = b + \sum_j w_jx_j\\\downarrow\\ y = b + \sum_i \color{red}{c_i}sigmoid(\color{green}{b_i} + \sum_j \color{blue}{w_{ij}}x_j)$$
</br>

ä¸‹é¢å¯¦éš›æ¼”ç¤ºäº†é€™å€‹æ¨¡å‹...
![](https://i.imgur.com/UQnR9LT.png) </br></br>
å¯ä»¥å°‡ $r_1$ã€ $r_2$ å’Œ $r_3$ çš„é‹ç®—ç°¡å¯«æˆå¦‚ä¸‹:
![](https://i.imgur.com/CvtALd5.png) </br></br>
æ¥è‘—ï¼Œå°‡ $r_i$ ä»£å…¥ $sigmoid$ å‡½æ•¸å¾—åˆ° $a_i$ (å¯è¡¨ç¤ºæˆ $a = \sigma(r)$ï¼Œ $a$ ã€ $r$ æ˜¯çŸ©é™£ï¼Œ $\sigma$ æ˜¯ $sigmoid$ )ï¼Œæœ€å¾ŒåŠ ä¸Š $b$ å¾—åˆ° $y$ ï¼Œå¦‚ä¸‹åœ–:
![](https://i.imgur.com/RH9MMiw.png) </br></br>
ç¶“éæ•´ç†å¾Œå¾—åˆ° $y$ çš„ç·šæ€§ä»£æ•¸è¡¨é”æ³•:
![](https://i.imgur.com/w42Xv6C.png) </br></br>
æ¥è‘—å°‡æ¯å€‹æœªçŸ¥åƒæ•¸çµ„æˆä¸€å€‹å¾ˆé•·çš„å‘é‡çŸ©é™£ $\theta$:
![](https://i.imgur.com/Dd67qNp.png) </br></br>
é€™æ¨£æˆ‘å€‘å°±é‡æ–°å®šç¾©äº†æ©Ÿå™¨å­¸ç¿’çš„ç¬¬ä¸€æ­¥
![](https://i.imgur.com/DkoyRrH.png)

### æ–°æ¨¡å‹çš„Losså‡½æ•¸ & æœ€ä½³åŒ–

#### Losså‡½æ•¸

ä»£å…¥çš„åƒæ•¸ç”±åŸæœ¬çš„ $w$ å’Œ $b$ æ›æˆäº† $\theta$
$$L(w, b)\rightarrow L(\theta)$$
![](https://i.imgur.com/CBf2PmY.png)
å¯ä»¥ç™¼ç¾ï¼Œé™¤äº†åƒæ•¸ä¸ä¸€æ¨£ä»¥å¤–ï¼Œå…¶ä»–éƒ½å’Œä¸Šé¢ä»‹ç´¹çš„Losså‡½æ•¸ä¸€æ¨£

#### æœ€ä½³åŒ–

$$
\theta^* = \arg\min\limits_{\theta}L,
\theta =
\left[
\begin{matrix}
& \theta_1\\
& \theta_2\\
& \theta_3\\
& \vdots &
\end{matrix}
\right]
$$

* (Randomly) Pick initial values $\theta^0$
* Compute gradient $g$

$$
g =
\left[
\begin{matrix}
& \frac{\delta L}{\delta\theta_1}|_{\theta = \theta^0} \\
& \frac{\delta L}{\delta\theta_2}|_{\theta = \theta^0} \\
& \vdots &
\end{matrix}
\right],
\left[
\begin{matrix}
& \theta_1^1 \\
& \theta_1^2 \\
& \vdots &
\end{matrix}
\right] \leftarrow
\left[
\begin{matrix}
& \theta_0^1 \\
& \theta_0^2 \\
& \vdots &
\end{matrix}
\right] -
\left[
\begin{matrix}
& \color{red}{\eta}\frac{\delta L}{\delta\theta_1}|_{\theta = \theta^0} \\
& \color{red}{\eta}\frac{\delta L}{\delta\theta_2}|_{\theta = \theta^0} \\
& \vdots &
\end{matrix}
\right]
$$
$$g = \nabla L(\theta^0), \theta^1 = \theta^0 - \color{red}{\eta}g$$

* ä¸åœåœ°åšç›´åˆ°gradientç‚ºé›¶å‘é‡ *(Zero Vector)* æˆ–ä¸æƒ³åšç‚ºæ­¢

**å¯¦ä½œä¸Šï¼Œæˆ‘å€‘æœƒå°‡Nç­†è³‡æ–™åˆ‡æˆä¸€å€‹ä¸€å€‹Batch...**
![](https://i.imgur.com/lomaHC8.png)
æ¯åšä¸€å€‹Batchå°±æœƒ**update**ä¸€æ¬¡ï¼ŒæŠŠæ‰€æœ‰Batchéƒ½åšå®Œä¸€è¼ªç¨±ç‚º**1 epoch**

:::success
:mag_right: **Example** </br>

#### Example 1

* 10000 examples (N = 10000)
* Batch size is 10 (B = 10)</br>
How many update in **1 epoch**?</br>
*A: 10000 / 10 = **1000 updates*** </br></br>

#### Example 2

* 1000 examples (N = 1000)
* Batch size is 100 (B = 100)</br>
How many update in **1 epoch**?</br>
*A: 1000 / 100 = **10 updates***
:::

### ReLU

å…¶å¯¦ä¸ä¸€å®šè¦åƒä¸Šé¢é€é**Sigmoid**å‡½æ•¸ä¾†è¿‘ä¼¼ï¼Œä¹Ÿå¯ä»¥å°‡
**<font color = blue>Hard Sigmoid</font>**
çœ‹æˆå…©å€‹ **Rectified Linear Unit (ReLU)** çš„åŠ ç¸½
![](https://i.imgur.com/2a4G8kX.png)
![](https://i.imgur.com/yAt23h2.png)
:::info
:information_source: é€™è£¡çš„ $\color{red}{2}i$ æ˜¯å› ç‚ºè¦åˆæˆ1æ¢**Hard Sigmoid**éœ€è¦2æ¢**ReLU**
:::
**<font color = blue>Q: é‚£é€™ä¸Šé¢å…©å€‹å“ªå€‹æ¯”è¼ƒå¥½å‘¢?</font>** </br>
*A: ReLUï¼Œè€å¸«æ¥ä¸‹ä¾†çš„å¯¦é©—éƒ½é¸æ“‡äº†ReLUï¼Œé¡¯ç„¶ReLUæ¯”è¼ƒå¥½ (è‡³æ–¼ç‚ºä»€éº¼ï¼Œä¹‹å¾Œå†è¬›)*

### å¯¦éš›å¯¦é©—çµæœ

![](https://i.imgur.com/LiT9EjM.png)
è¶Šå¤šçš„ReLUå¯ä»¥è£½é€ è¶Šè¤‡é›œçš„æ›²ç·šï¼Œä¸éåˆ°äº†1000å€‹ReLUä¹‹å¾Œï¼Œé›–ç„¶åœ¨è¨“ç·´è³‡æ–™ä¸Šæœ‰æ›´ä½çš„Lossï¼Œä½†åœ¨é æ¸¬ä¸Šå°±æ²’æœ‰å¤ªå¤§çš„é€²æ­¥</br></br>
æ¥è‘—æˆ‘å€‘ç¹¼çºŒæ”¹é€²æ¨¡å‹...
![](https://i.imgur.com/v6GvCLS.png)
å¯ä»¥å¤šåšå¹¾æ¬¡: $x\rightarrow a\rightarrow a'\rightarrow \cdots$
é€™å€‹ä¹Ÿæ˜¯**Hyper Parameter**ï¼Œè¦è‡ªå·±æ±ºå®šåšå¹¾å±¤ï¼Œä¸‹é¢æœ‰å¯¦éš›çš„å¯¦é©—æ•¸æ“š...</br></br>
![](https://i.imgur.com/slfgKXg.png)
å¢åŠ å±¤æ•¸ï¼Œåœ¨è¨“ç·´è³‡æ–™ä¸ŠLossæœ‰é¡¯è‘—çš„é™ä½ï¼Œåœ¨é æ¸¬ä¸Šä¹Ÿæœ‰é€²æ­¥ </br></br>
![](https://i.imgur.com/QPVUxzF.png)
å¯ä»¥ç™¼ç¾åœ¨å…©å‘¨ä¸€æ¬¡çš„ä½è°·çš„åœ¨é æ¸¬ä¸Šç®—æ˜¯è »ç²¾æº–çš„ï¼Œä¸éé€™è£¡æœ‰å€‹å¾ˆæœ‰è¶£çš„åœ°æ–¹ï¼Œä¹Ÿå°±æ˜¯åœ¨ <font color = red>?</font> çš„åœ°æ–¹ï¼Œåš´é‡é«˜ä¼°äº†è§€çœ‹æ•¸å€¼ï¼Œé€™å…¶å¯¦ä¹Ÿä¸å¤ªèƒ½æ€ªå®ƒé æ¸¬çš„ä¸ç²¾æº–ï¼Œé€™ä¸€å¤©å…¶å¯¦æ˜¯é™¤å¤•ï¼Œæ‰€ä»¥è§€çœ‹æ•¸æ¯”é æœŸçš„ä½å¾ˆå¤š

### ç¥ç¶“ç¶²è·¯ & æ·±åº¦å­¸ç¿’

![](https://i.imgur.com/8YxmaY4.png)
:::info
Many Neuron $\rightarrow$ Neuron Network</br>
Many hidden layer $\rightarrow$ Deep $\rightarrow$ Deep Learning
:::
æ–¼æ˜¯äººå€‘æŠŠé¡ç¥ç¶“ç¶²è·¯è¶Šç–Šè¶Šå¤šã€è¶Šç–Šè¶Šæ·±...
![](https://i.imgur.com/9Oscfvl.png)
![](https://i.imgur.com/gvLfs77.png)
:::success
ä¸éè¦è¨“ç·´é€™éº¼æ·±çš„Networkæ˜¯æœ‰è¨£ç«…çš„ï¼Œé€™å€‹ä¹‹å¾Œå†è¬›...
:::

**Q: è¦é€¼è¿‘ä¸€å€‹è¤‡é›œçš„å‡½æ•¸ï¼Œå¯¦éš›ä¸Šåªè¦æœ‰å¤ å¤šçš„ReLUå’ŒSigmoidå°±å¯ä»¥é€¼è¿‘ä»»ä½•çš„é€£çºŒå‡½æ•¸ï¼Œç†è«–ä¸Šåªè¦ä¸€æ’ReLUæˆ–Sigmoidå¤ å¤šå°±è¶³å¤ äº†ï¼Œç‚ºä½•è¦æ·±å‘¢? èƒ–ä¸è¡Œå—? åªæ˜¯å–®ç´”"Deep" Networkæ¯”"Fat" Networkçœ‹èµ·ä¾†æ›´å²å®³å—? ç‚ºä½•æˆ‘å€‘ä¸è¦æŠŠNetworkè®Šèƒ–ï¼Œè€Œæ˜¯è®Šæ·±å‘¢?**</br>
*A: å¥½å•é¡Œ! ä¹‹å¾Œæœƒå†è¬›*

**Q: Deep Networkè¶Šæ·±å°±ä¸€å®šè¶Šå¥½å—?**</br>
*A: ä¸ä¸€å®šï¼Œæœ‰å¯èƒ½æœƒå‡ºç¾ **Overfitting(éåº¦æ“¬åˆ)** çš„ç¾è±¡*
![](https://i.imgur.com/ebPGDAX.png)

## æ©Ÿå™¨å­¸ç¿’ä»»å‹™æ”»ç•¥

```mermaid
graph TD

A([loss on training data])
B([Î£OAO!!])
C([loss on testing data])
D([make your model complex])
E([Next Lecture])
F([Î£OAO!!])
G([:D])
H([more training data argumentation make your model simpler])
I([Not in HWs, except HW 11])

A -->|large| B
A -->|small| C
B -->|model bias| D
B -->|optimization| E
C -->|large| F
C -->|small| G
F -->|overfitting| H
F -->|mismatch| I
D ---|need trade-off| H
```

:::info
:information_source: trade-off: split your training data into training set and validation set for model selection
:::

</br>

:::warning
:pushpin: **ç‹€æ³1: è¨“ç·´è³‡æ–™çš„Losså°±å¾ˆå¤§!!** </br>
*å¯èƒ½æ˜¯Model Biasæˆ–Optimization Issue*
:::

### Model Bias

é€™å€‹ç‹€æ³æ˜¯Modelå¤ªéç°¡å–®äº†ï¼Œå°è‡´åœ¨é€™å€‹ä¸€å †å‡½æ•¸æ‰€æˆçš„é›†åˆè£¡é¢ï¼Œå³ä¾¿æ˜¯æœ€å¥½ *(è®“Lossæœ€ä½)* çš„å‡½æ•¸Lossä¹Ÿé‚„æ˜¯å¾ˆé«˜ã€‚æ›å¥è©±èªªï¼Œå°±æ˜¯å¯ä»¥è®“Lossè®Šä½çš„å‡½æ•¸ï¼Œä¸åœ¨é€™å€‹modelå¯ä»¥æè¿°çš„ç¯„åœä¹‹å…§ã€‚ä¸‹åœ–æ¸…æ¥šçš„è¡¨ç¤ºäº†é€™å€‹ç‹€æ³...
:::info
:paperclip: è€å¸«é€™è£¡ä¸‹äº†ä¸€å€‹æ¯”å–»: å°±å¥½æ¯”ä½ åœ¨å¤§æµ·æ’ˆé‡ï¼Œä½†æµ·è£¡æ ¹æœ¬æ²’æœ‰é‡
:::
![](https://i.imgur.com/dpDi3zI.png)
:::success
:heavy_check_mark: **è§£æ±ºæ–¹æ³•:** é‡æ–°è¨­è¨ˆmodelè®“å®ƒæ›´æœ‰ "å½ˆæ€§" !</br></br>
![](https://i.imgur.com/Iy9MJc1.png)
:::

### Optimization Issue

ç›®å‰æˆ‘å€‘åªå­¸åˆ°gradient descentçš„æ–¹æ³•åšoptimizationï¼Œé€™å€‹æ–¹æ³•æœ‰å€‹é¡¯è€Œæ˜“è¦‹çš„å•é¡Œ: é€šå¸¸ç„¡æ³•æ‰¾åˆ°golbal minima! </br>
![](https://i.imgur.com/7oOEN6C.png) </br>
é€™å€‹modelè£¡é¢ç¢ºå¯¦æœ‰å­˜åœ¨ä¸€å€‹å‡½æ•¸çš„Lossæ˜¯å¤ ä½çš„ï¼Œä½†gradient descentå»æ²’æœ‰çµ¦æˆ‘å€‘é€™å€‹å‡½æ•¸
:::info
:paperclip: è€å¸«é€™è£¡ä¹Ÿä¸‹äº†ä¸€å€‹æ¯”å–»: å°±å¥½æ¯”ä½ åœ¨å¤§æµ·æ’ˆé‡ï¼Œé‡ç¢ºå¯¦åœ¨æµ·è£¡ï¼Œä½†æˆ‘å€‘å»æ²’è¾¦æ³•æŠŠé‡æ’ˆèµ·ä¾†(æ‰¾ä¸åˆ°QAQ)
:::
![](https://i.imgur.com/0AyGpo1.png)

### Model Bias v.s. Optimization Issue

é€™å€‹æ™‚å€™å°±æœƒé–‹å§‹å¶å–Šï¼Œè›¤æ‰€ä»¥åˆ°åº•æ˜¯å“ªå€‹ç‹€æ³å•Š? åˆ°åº•æ˜¯modelä¸å¤ å¤§ï¼Œé‚„æ˜¯modelå¤ å¤§äº†ï¼Œåªæ˜¯æˆ‘å€‘æ²’è¾¦æ³•æ‰¾åˆ°å¤ å¥½çš„å‡½æ•¸? è©²æ€éº¼åˆ¤æ–·å‘¢?
:::success
å¯ä»¥é€éæ¯”è¼ƒä¸åŒçš„modelä¾†å¾—çŸ¥èªªmodelåˆ°åº•å¤ ä¸å¤ å¤§!
:::
å…ˆä¾†çœ‹ä¸‹é¢çš„å¯¦ä¾‹...
![](https://i.imgur.com/83frpJu.png)
è©¦è‘—åˆ†æä¸€ä¸‹ï¼Œé¦–å…ˆæˆ‘å€‘çœ‹åˆ°åœ¨Testing Dataé€™æ–¹é¢ï¼Œ56-layerçš„Lossæ¯”20-layerçš„Lossé«˜ï¼Œå…ˆåˆ¥ä»¥ç‚ºå°±æ˜¯overfittingï¼Œæˆ‘å€‘å†çœ‹ä¸€ä¸‹Training Dataï¼Œç™¼ç¾56-layerçš„Lossé‚„æ˜¯æ¯”20-layerä¾†å¾—é«˜ï¼Œå¦‚æœæ˜¯overfittingç†è«–ä¸Š56-layeråœ¨Training Dataä¸Šæ‡‰è©²è¦æ¯”20-layeræœ‰æ›´ä½çš„Lossï¼Œ==ä½†é€™å€‹ç‹€æ³å»æ˜¯ä¸ç®¡Testing Dataé‚„æ˜¯Training Dataéƒ½æ˜¯56-layeræœ‰æ›´é«˜çš„Lossï¼Œä»£è¡¨é€™æ˜¯Optimization Issueï¼Œ56-layeræ²’åšå¥½optimizationï¼Œæ‰¾ä¸åˆ°æ›´ä½Lossçš„å‡½æ•¸ï¼Œæ‰€ä»¥Lossæ‰æœƒæ¯”20-layeré«˜!==
:::info
:bulb: **è€å¸«çš„å»ºè­°** </br>
é‡åˆ°æ²’åšéçš„å•é¡Œï¼Œå¯ä»¥å…ˆè·‘ä¸€äº›æ¯”è¼ƒå°ã€æ¯”è¼ƒæ·ºçš„networkï¼Œæˆ–æ˜¯ç”¨ä¸€äº›ä¸æ˜¯deep learningçš„æ–¹æ³• *(e.g. linear model, support vector machine...)* ï¼Œé€™äº›modelæ˜¯æ¯”è¼ƒå®¹æ˜“åšoptimizeçš„ï¼Œæ¯”è¼ƒä¸æœƒæœ‰optimizationå¤±æ•—çš„å•é¡Œï¼Œä¹‹å¾Œä¾¿æ–¼å’Œæ·±çš„modelæ¯”è¼ƒLoss
:::
é€™ä¹Ÿæ˜¯Optimization Issueï¼Œç™¼ç”Ÿåœ¨5 layer
![](https://i.imgur.com/wgjQn1k.png)
:::success
:heavy_check_mark: **è§£æ±ºæ–¹æ³•:** æ›´å¼·å¤§çš„OptimizationæŠ€è¡“!! *(ä¸‹ä¸€ç¯€èª²å†è¬›)*
:::

### Overfitting & Mismatch

:::warning
:pushpin: **ç‹€æ³2: è¨“ç·´è³‡æ–™çš„Losså°ï¼Œä½†æ˜¯æ¸¬è©¦è³‡æ–™çš„Losså¤§!!**</br>
*å¯èƒ½æ˜¯Overfittingæˆ–Mismatch*
:::
ä¸‹é¢èˆ‰ä¸€å€‹æ¥µç«¯çš„ä¾‹å­...
:::success
:mag_right: **An extreme example** </br>
Training data:
$$\{(x^1, \hat{y^1}), (x^2, \hat{y^2}), \dots, (x^N, \hat{y^N})\}$$
Model:
$$
f(x) =
\begin{cases}
\hat{y^i} & \exists x^i = x \\
random & otherwise
\end{cases}
$$
This function obtains **zero training loss**, but **large testing loss.**
:::
ä¸Šé¢å¯ä»¥çœ‹åˆ°ï¼Œé€™å€‹å‡½æ•¸ç°¡ç›´ä¸€ç„¡æ˜¯è™•ï¼Œå¦‚æœè¨“ç·´è³‡æ–™æœ‰ç›¸åŒçš„ $x^i$ å°±è¼¸å‡ºè·Ÿè¨“ç·´è³‡æ–™ä¸€æ¨¡ä¸€æ¨£çš„ $\hat{y^i}$ ï¼Œå¦‚æœæ²’æœ‰ï¼Œå°±éš¨æ©Ÿè¼¸å‡ºã€‚é›–ç„¶åœ¨training dataä¸Šçš„lossæ˜¯0ï¼Œä½†æ‹¿åˆ°testing dataä¸Šçš„è¡¨ç¾æ˜¯æ¥µç‚ºç³Ÿç³•çš„</br></br>

**æ—¥å¸¸æœƒé‡åˆ°çš„ä¾‹å­:**
![](https://i.imgur.com/0TRvDxM.png)
å¤ªéæœ‰å½ˆæ€§çš„modelåœ¨æ²’æœ‰è¨“ç·´åˆ°çš„åœ°æ–¹å°±æœƒæœ‰ **"freestyle"** å°è‡´åœ¨Testing dataæœ‰è¼ƒå¤§çš„Loss
:::success
:heavy_check_mark: **è§£æ±ºæ–¹æ³•:** </br>

1. å¢åŠ è¨“ç·´è³‡æ–™! </br>*<font color = gray>åªè¦æœ‰æ›´å¤šçš„è¨“ç·´è³‡æ–™å°±èƒ½é™åˆ¶ä½å‡½æ•¸çš„å½¢ç‹€ï¼Œæ¸›å°‘"freestyle"çš„ç™¼ç”Ÿ</font>*</br>*<font color = gray>(</font><font color = red>ä¸å¯</font><font color = gray>åœ¨ä½œæ¥­ä¸­ä½¿ç”¨)</font>*</br>ä½†å¯ä»¥ä½¿ç”¨: Data Augmentation </br>*<font color = gray>e.g. å°‡åœ–ç‰‡å·¦å³ç¿»è½‰ã€æˆªä¸€å¡Šå‡ºä¾†æ”¾å¤§...(ä½†æ˜¯è¦åˆç†)</font>*
![](https://i.imgur.com/MpzoSI2.png) </br></br>
2. çµ¦æ¨¡å‹ä¸€äº›é™åˆ¶! </br>
*<font color = gray>è¦çµ¦å¤šå°‘é™åˆ¶å–æ±ºæ–¼è‡ªå·±å°å•é¡Œçš„ç†è§£ ~~(è‡ªå·±é€šéˆ)~~</font>*
![](https://i.imgur.com/tfp2BSP.png)</br></br>
*<font color = gray>é™åˆ¶çš„æ–¹æ³•:</font>* </br>
<font color = gray>

    * Less parameters, sharing parameters
    * Less features
    * Early stopping
    * Regularization
    * Dropout

</font>
:::

**CNN** </br>
![](https://i.imgur.com/DIPvlit.png) </br>
ç›¸æ¯”ä¸€èˆ¬çš„Fully-connectedæ¶æ§‹ï¼ŒCNNçš„é™åˆ¶è¼ƒå¤šï¼Œå®ƒå¯ä»¥æ‰¾åˆ°å‡½æ•¸è¼ƒå°‘ï¼Œä½†å®ƒå› ç‚ºé‡å°å½±åƒçš„ç‰¹æ€§ä¾†é™åˆ¶modelï¼Œæ‰€ä»¥CNNåœ¨å½±åƒä¸Šçš„è¡¨ç¾è¼ƒå¥½
:::danger
:x: **æ³¨æ„ é™åˆ¶ä¸å¯éå¤š** </br>
é™åˆ¶éå¤š $\rightarrow$ model biaså•é¡Œ </br>
![](https://i.imgur.com/BC6roFk.png)
:::

**Bias-Complexity Trade-off**
![](https://i.imgur.com/yZHIZLO.png)

**Cross Validation** </br>
ç‚ºäº†é¿å…ç³¾çµåœ¨public testing setä¸Šé¢ï¼Œæ‡‰è©²å°‡training setæ‹†å‡ºvalidation set *(é€šå¸¸9:1)* æ ¹æ“švalidation setå‡ºä¾†çš„lossä¾†æŒ‘é¸æ¨¡å‹
![](https://i.imgur.com/6V0nn45.png)
ä½†æ˜¯é€™æ¨£å¯èƒ½æœƒé‡åˆ°ä¸€å€‹å•é¡Œï¼Œå¦‚æœåˆ†å¾—ä¸å¥½ï¼Œå‰›å‰›å¥½å¥‡æ€ªçš„dataéƒ½åˆ†åˆ°validation setï¼Œå°è‡´çµæœå¾ˆå·®æ€éº¼è¾¦? $\rightarrow$ **N-fold Cross Validation** !!

**N-fold Cross Validation** </br>
å…ˆå°‡training setåˆ†æˆNç­‰ä»½ï¼Œå°‡å…¶ä¸­ä¸€ä»½æ‹¿ä¾†ç•¶validation setï¼Œå…¶é¤˜ç•¶training setï¼Œé‡è¤‡Næ¬¡ï¼Œå°‡æ¯å€‹modelæ¯æ¬¡çš„lossåŠ ç¸½èµ·ä¾†å¹³å‡ä¸¦æ¯”è¼ƒï¼Œæ‰¾åˆ°lossæœ€å°çš„modelï¼Œæœ€å¾Œå†æŠŠæ‰€æœ‰çš„training setçµ¦lossæœ€ä½çš„modelï¼Œå°±å¯ä»¥æ‹¿å»è·‘testing setäº†ï¼Œä¸‹åœ–ä»¥N = 3èˆ‰ä¾‹èªªæ˜...
![](https://i.imgur.com/XAQebmv.png)

### Mismatch

è¨“ç·´è³‡æ–™ å’Œ æ¸¬è©¦è³‡æ–™ åˆ†ä½ˆä¸ä¸€æ¨£ *<font color = gray>(å¤§éƒ¨åˆ†ä½œæ¥­ä¸æœƒé‡åˆ°é€™ç¨®å•é¡Œï¼Œé™¤äº†HW11)</font>*
![](https://i.imgur.com/Xs846ht.png)

## é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ (ä¸€)ï¼š å±€éƒ¨æœ€å°å€¼ (local minima) èˆ‡éé» (saddle point)

### ç‚ºä»€éº¼Optimizationæœƒå¤±æ•—?

éš¨è‘—åƒæ•¸ä¸æ–·updateï¼Œtraining losså»ä¸å†ä¸‹é™è‡³æ»¿æ„çš„æ•¸å€¼ï¼Œç”šè‡³æ˜¯ä¸€é–‹å§‹modelå°±è¨“ç·´ä¸èµ·ä¾†...$\rightarrow$ èµ°åˆ°äº†ä¸€å€‹åœ°æ–¹ï¼Œåƒæ•¸å°lossçš„å¾®åˆ†ç‚º0ï¼Œä½¿å¾—gradient descentæ²’è¾¦æ³•å†updateåƒæ•¸
![](https://i.imgur.com/3fre5ie.png)
:::warning
:warning: **æ³¨æ„** </br>
é€™å€‹æ™‚å€™é€šå¸¸æœƒæƒ³åˆ°local minimaå•é¡Œï¼Œä½†äº‹å¯¦ä¸Šä¸¦ä¸æ˜¯åªæœ‰local minimaçš„gradientæ˜¯0ï¼Œæ¯”å¦‚èªª: **éé» (saddle point)**ï¼Œé€™äº›gradientæ˜¯0çš„åœ°æ–¹ï¼Œæˆ‘å€‘é€šç¨± **é—œéµé» (critical point)**
:::
![](https://i.imgur.com/ZLGLis4.png)

* **local minima:** :x: no way to go
* **saddle point:** :heavy_check_mark: can escape

### åˆ¤æ–· Local Minima å’Œ Saddle Point

æƒ³è¦çŸ¥é“ä¸€å€‹é»åˆ°åº•æ˜¯local minimaé‚„æ˜¯saddle point $\rightarrow$ è¦çŸ¥é“loss functionçš„å½¢ç‹€ $\rightarrow$ ä½†loss functionæ¥µç‚ºè¤‡é›œï¼Œè¦å¦‚ä½•çŸ¥é“å®ƒçš„å½¢ç‹€å‘¢?
![](https://i.imgur.com/HJYxaO6.png)
é›–ç„¶ä¸çŸ¥é“ $L(\theta)$ çš„æ¨£å­ï¼Œä½†å¯ä»¥å¾ $L(\theta')$ å»é€¼è¿‘å®ƒ </br></br>
![](https://i.imgur.com/5OHPWxu.png)
åœ¨critical pointçš„æ™‚å€™ï¼Œgradientç‚º0ï¼Œå› æ­¤ $(\theta - \color{blue}{\theta'})^T\color{green}{g}$ é€™ä¸€é …ç‚º0ï¼Œæˆ‘å€‘å¯ä»¥æ ¹æ“š $\frac{1}{2}(\theta - \color{blue}{\theta'}^T)\color{red}{H}(\theta - \color{blue}{\theta'})$ é€™é …ä¾†åˆ¤æ–· $\color{blue}{\theta'}$ é™„è¿‘çš„error surfaceé•·ä»€éº¼æ¨£å­ï¼Œé€²è€ŒçŸ¥é“ $\color{blue}{\theta'}$ æ˜¯local minimaé‚„æ˜¯saddle point</br></br>

:::info
[:information_source: å¦‚ä½•è¨ˆç®—çŸ©é™£çš„ç‰¹å¾µå€¼(eigen values)](https://silverwind1982.pixnet.net/blog/post/154593170)
:::

**ç”±Hessianåˆ¤æ–·é—œéµé»:** </br>

![](https://i.imgur.com/fT61Eb4.png)
:::success
:mag_right: **Example** </br></br>
![](https://i.imgur.com/DL6UgtR.png)
![](https://i.imgur.com/7MQ3h8Z.png) </br></br>
**$\lambda_1, \lambda_2$ çš„è¨ˆç®—éç¨‹:**
$$
\begin{align*}
&\color{red}{H} =
\left[
\begin{matrix}
& 0 & -2 \\
& -2 & 0 &
\end{matrix}
\right] \\
&det{
\left(
\left[
\begin{matrix}
& 0 - \lambda & -2 \\
& -2 & 0 - \lambda &
\end{matrix}
\right]
\right)
} = 0
\end{align*}
$$
$$
\begin{align*}
âˆ´& \lambda^2 - 4 = 0, \quad\lambda^2 = 4, \quad\lambda = \pm 2 \\
âˆ´& \lambda_1 = 2, \quad\lambda_2 = -2
\end{align*}
$$
:::
</br>

**åƒæ•¸å¯ä»¥updateçš„æ–¹å‘:** </br>
![](https://i.imgur.com/pfVYuz7.png)
ç°¡å–®ä¾†èªªï¼Œ $\theta = \color{blue}{\theta'} + u$ å°±å¯ä»¥è®“lossè®Šå°
:::success
:mag_right: **Example** </br></br>
![](https://i.imgur.com/TOPpcJH.png)
:::
:::danger
:exclamation: ä½†æ˜¯å› ç‚ºä¸Šè¿°æ–¹æ³•çš„é‹ç®—é‡å·¨å¤§ï¼Œæ‰€ä»¥å¹¾ä¹ä¸æœƒåœ¨å¯¦ä½œä¸­ä½¿ç”¨
:::

### Saddle Point v.s. Local Minima

**Q: æ—¢ç„¶saddle pointæ¯”è¼ƒä¸å¯æ€•ï¼Œé‚£æˆ‘å€‘å¦‚æœæ¯”è¼ƒå¸¸é‡åˆ°çš„æ˜¯saddle pointï¼Œæ˜¯ä¸æ˜¯å°±èƒ½æ¯”è¼ƒä¸ç”¨æ“”å¿ƒ? åˆ°åº•å“ªå€‹å¸¸è¦‹å‘¢?** </br>
*A: [æˆ‘å€‘å…ˆé€²ä¸€æ®µå°æ•…äº‹! (é­”æ³•å¸«ç‹„å¥§å€«å¨œ)](https://aijianggu.com/collect/835222.html)*
</br></br>
é€™å€‹æ•…äº‹çµ¦æˆ‘å€‘äº†ä¸€å€‹å•Ÿç™¼: ä¸‰ç¶­ç©ºé–“ç„¡è·¯å¯èµ°çš„åœ°æ–¹ï¼Œåœ¨å››ç¶­æˆ–æ˜¯æ›´é«˜ç¶­çš„ç©ºé–“æ˜¯ä¸æ˜¯æœ‰å¯èƒ½é‚„æœ‰è·¯å¯ä»¥èµ°å‘¢?

![](https://i.imgur.com/fqHKRXF.png)
å¦‚åœ–ï¼Œåœ¨äºŒç¶­ç©ºé–“ä¸­çœ‹ä¼¼local minimaçš„åœ°æ–¹ï¼Œåœ¨ä¸‰ç¶­ç©ºé–“ä¸­å»å¯èƒ½æ˜¯saddle pointï¼Œåœ¨å‹•è¼’æœ‰ç™¾è¬ã€åƒè¬åƒæ•¸ *(ç™¾è¬ã€åƒè¬ç¶­åº¦)* çš„modelä¸­ï¼Œæ˜¯ä¸æ˜¯å…¶å¯¦æœ‰å¾ˆå¤šè·¯å¯ä»¥èµ°å‘¢? </br>

**çœŸå¯¦å¯¦é©—æ¡ˆä¾‹:**
![](https://i.imgur.com/HDet9Ye.png)
å¯ä»¥ç™¼ç¾ï¼Œå³ä½¿æ˜¯åœ¨æœ€æ¥µç«¯çš„ç‹€æ³ä¸‹ï¼Œé‚„æ˜¯æœ‰å¹¾ä¹ä¸€åŠçš„è·¯æ˜¯å¯ä»¥è®“lossä¸‹é™çš„ï¼Œç„¡è·¯å¯èµ°(local minima)çš„ç‹€æ³ä¸¦ä¸å¸¸è¦‹

## é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ (äºŒ)ï¼š æ‰¹æ¬¡ (batch) èˆ‡å‹•é‡ (momentum)

### Batch

:::info
:paperclip: **è¤‡ç¿’** </br></br>
![](https://i.imgur.com/lomaHC8.png)
:::
1 **epoch** = see all the batches once $\rightarrow$ **Shuffle** after each epoch *(æ¯å€‹epochéƒ½æœƒé‡æ–°åˆ†batch)*
![](https://i.imgur.com/1mCHgAx.png)
å¾é€™ä¸€å¼µåœ–æˆ‘å€‘å¯ä»¥çœ‹åˆ°ï¼Œå¤§çš„Batché›–ç„¶éœ€è¦çš„æ™‚é–“æ¯”è¼ƒä¹…ï¼Œä½†å»æ¯”è¼ƒç²¾æº–ï¼Œè€Œå°çš„Batchéœ€è¦çš„æ™‚é–“æ¯”è¼ƒå°ï¼Œä½†æ¯”è¼ƒä¸æº–ï¼Œä¸ä¸€å®šæœƒæœè‘—æ­£ç¢ºçš„æ–¹å‘å‰é€²ã€‚**ä½†çœŸçš„æ˜¯é€™æ¨£å—?ï¼Œä½ å¯èƒ½æ²’è€ƒæ…®åˆ°<font color = red>å¹³è¡Œè™•ç†</font>çš„å•é¡Œ!!** </br></br>
![](https://i.imgur.com/GSHORCy.png) </br></br>
æ­¤ç‚ºå¯¦éš›çš„å¯¦é©—çµæœï¼Œå› ç‚ºGPUå¯ä»¥å¹³è¡Œé‹ç®—ï¼Œæ‰€ä»¥ç•¶batch sizeæ˜¯1 ~ 1000æ™‚ï¼Œæ‰€éœ€è¦çš„æ™‚é–“æ˜¯å¹¾ä¹ç›¸åŒçš„ï¼Œä¸éGPUå¹³è¡Œé‹ç®—çš„èƒ½åŠ›çµ‚ç©¶æœ‰é™ï¼Œæ‰€ä»¥ç•¶batch sizeå¤§åˆ°è¶…éä¸€å®šçš„å¤§å°ä¹‹å¾Œï¼Œéœ€è¦çš„æ™‚é–“é‚„æ˜¯æœƒéš¨è‘—batch sizeçš„å¢å¤§è€Œå¢åŠ çš„ </br></br>
![](https://i.imgur.com/WMBHTUc.png) </br></br>
å› æ­¤ï¼Œå¤§çš„batch sizeåè€Œæ˜¯æ¯”è¼ƒæœ‰æ•ˆç‡çš„ï¼Œæ‰€ä»¥æˆ‘å€‘å›åˆ°ä¸Šé¢çš„é€™ä¸€å¼µåœ–:
![](https://i.imgur.com/DhLxMGz.png) </br></br>
é€™æ¨£çœ‹èµ·ä¾†ï¼Œå¤§çš„Batchçš„åŠ£å‹¢æ¶ˆå¤±äº†ï¼Œè€Œå°çš„Batchçš„å„ªå‹¢æ²’äº†ã€‚é€™æ¨£çœ‹èµ·ä¾†ä¼¼ä¹å¤§çš„Batchæ˜¯æ¯”è¼ƒå¥½çš„ï¼Œä½†é€™è£¡åˆå‡ºç¾äº†ä¸€å€‹åç›´è¦ºçš„åœ°æ–¹äº†ï¼Œ**<font color = red>noisy</font>çš„gradientåè€Œå¯ä»¥å¹«åŠ©train** </br></br>
![](https://i.imgur.com/PVzY9fX.png)

* Smaller batch size has better performance
* What wrong with large batch size? $\rightarrow$ Optimization Fails
</br>

![](https://i.imgur.com/8uGSzHD.png)
åŸå› åœ¨æ–¼Full Batchçš„æ™‚å€™ï¼ŒLosså‡½æ•¸æ˜¯å›ºå®šçš„ï¼Œç•¶èµ°åˆ°critical pointçš„æ™‚å€™å°±å®¹æ˜“å¡ä½ï¼Œç„¡æ³•å†å¾—åˆ°æ›´ä½çš„lossï¼Œä½†Small Batchä¸ä¸€æ¨£ï¼Œä»–æ¯æ¬¡çš„Losså‡½æ•¸éƒ½æœ‰äº›å¾®çš„ä¸åŒï¼Œå¦‚åœ–ï¼Œé›–ç„¶åœ¨<font color = green>$L^1$</font>å¡ä½äº†ä½†åœ¨<font color = blue>$L^2$</font>å°±æœ‰å¯èƒ½å¯ä»¥ç¹¼çºŒtrainä¸‹å» </br></br>

![](https://i.imgur.com/aL5CNJ2.png)
æ¥ä¸‹ä¾†å°±æ˜¯æ›´å¼”è©­çš„äº‹äº†ï¼Œç¶“éå¯¦é©—å¾—åˆ°çš„æ•¸æ“šä¾†çœ‹ï¼Œ==Small Batchå°æ–¼testingä¸Šæœ‰æ›´å¥½çš„è¡¨ç¾==ï¼Œå³ä¾¿æˆ‘å€‘åŠªåŠ›è®“Large Batchåœ¨trainingä¸Šè·ŸSmall Batchä¸Šæœ‰ç›¸ä¼¼çš„Accuracyï¼Œä½†Large Batchåœ¨testingä¸Šé‚„æ˜¯è¼¸çµ¦äº†Small Batchï¼Œä¹Ÿå°±æ˜¯Large Batchä¸Šå‡ºç¾äº†overfittingï¼Œç‚ºä»€éº¼æœƒé€™æ¨£å‘¢? </br></br>
![](https://i.imgur.com/kn2Qy5W.png)
é€™æ˜¯å› ç‚ºåŒæ¨£æ˜¯Minimaï¼Œæœ‰å¯èƒ½æ˜¯Flat Minimaï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯Sharp Minimaï¼Œå¦‚æœtraining dataå’Œtesting dataä¸Šæœ‰mismatchï¼Œåœ¨Flat Minimaä¸Šçš„å½±éŸ¿å¯èƒ½å°±ä¸å¤ªå¤§ï¼Œé‚„æ˜¯æœƒæœ‰å¾ˆå¥½çš„è¡¨ç¾ï¼Œä½†å°æ–¼Sharp Minimaï¼Œä¸€é»é»çš„mismatchå°±æ˜¯éå¸¸è‡´å‘½çš„ã€‚è€Œå¾ˆå¤šäººç›¸ä¿¡ï¼ŒSmall Batchæ¯”èµ·Large Batchï¼Œæ›´å®¹æ˜“è·‘åˆ°Flat Minimaï¼Œä¸€å€‹æ¯”è¼ƒç›´è§€çš„è§£é‡‹æ˜¯ï¼Œnoisyçš„gradientåœ¨Sharp Minimaå¯èƒ½ä¸€å€‹ä¸å°å¿ƒå°±è·³é›¢é–‹äº†ï¼Œè€ŒFlat Minimaæ‰æ¯”è¼ƒå®¹æ˜“å›°ä½å®ƒ *<font color = gray>(ä½†é€™å€‹é‚„æ˜¯å€‹å°šå¾…ç ”ç©¶çš„å•é¡Œï¼Œé€™å€‹è§£é‡‹ä¹Ÿä¸æ˜¯100%æ­£ç¢º)</font>*
:::success
:memo: **Summary** </br>
ç¸½çµä¸€ä¸‹ï¼ŒSmall Batchå’ŒLarge Batchå„æœ‰å„ªç¼ºé»ï¼Œæ‰€ä»¥é€™ä¹Ÿæ˜¯ä¸€å€‹è¦è‡ªå·±è¨­å®šçš„hyperparameter
![](https://i.imgur.com/6w47SXm.png)
:::

### Momentum

å‡è¨­error surfaceå¦‚ä¸‹åœ–ä¸­çš„æ–œå¡ï¼Œåœ¨ç‰©ç†çš„ä¸–ç•Œä¸­ï¼Œçƒä¸ä¸€å®šæœƒæ»¾åˆ°critical pointå°±åœä¸‹ä¾†ï¼Œè€Œæ˜¯æœƒç¹¼çºŒæ»¾ä¸‹å»ï¼Œç”šè‡³å‹•é‡è¶³å¤ çš„è©±ï¼Œé‚„å¯èƒ½å¯ä»¥è¶Šéå°å±±ä¸˜ï¼Œå¾€æ›´ä½è™•å‰é€²
![](https://i.imgur.com/vG8dFUT.png) </br></br>
**ä¸€èˆ¬Gradient Descent:** </br>
åŸºæœ¬ä¸Šå°±æ˜¯ä¸€ç›´é‡è¤‡ç®—å‡ºgradientï¼Œç„¶å¾Œå¾€gradientçš„åæ–¹å‘ç§»å‹•
![](https://i.imgur.com/GIhMoLY.png) </br></br>
**Gradient + Gradient Descent:** </br>
ä¸åƒ…åƒ…è€ƒæ…®ç®—å‡ºä¾†çš„gradientï¼Œé‚„è¦è€ƒæ…®ä¸Šä¸€æ­¥!
![](https://i.imgur.com/SwtyPA5.png) </br></br>

æ•´ç†ä¸€ä¸‹ä¸Šé¢çš„å¼å­... </br>
$m^i$ is the weighted sum of all the pervious gradient: $g^0, g^1, \dots, g^{i-1}$
$$
\begin{align*}
m^0 &= 0 \\
m^1 &= -\eta g^0 \\
m^2 &= -\lambda\eta g^0 - \eta g^1
\end{align*}
$$
</br>

æ›æˆä¸€é–‹å§‹çš„åœ–åƒä¾†çœ‹çš„è©±ï¼Œç¢ºå¯¦æœ‰é»åƒç‰©ç†çš„å°çƒé‚£æ¨£ï¼Œæœ‰æ©Ÿæœƒæ»¾åˆ°æ›´ä½çš„åœ°æ–¹
![](https://i.imgur.com/YWNxanc.png)

## é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ (ä¸‰)ï¼šè‡ªå‹•èª¿æ•´å­¸ç¿’é€Ÿç‡ (Learning Rate)

### å›ºå®šçš„ Learning Rate

ä¸Šé¢è¬›åˆ°äº†critical pointçš„å•é¡Œï¼Œä½†å…¶å¯¦åœ¨è¨“ç·´Networkçš„æ™‚å€™ï¼Œcritical pointå¾€å¾€ä¸æ˜¯æœ€å¤§çš„éšœç¤™...
![](https://i.imgur.com/o3jNWTA.png)
ç•¶æˆ‘å€‘çœ‹åˆ°lossä¸å†ç¹¼çºŒå¾€ä¸‹æ‰çš„æ™‚å€™ï¼Œå¯èƒ½æœƒä»¥ç‚ºæ˜¯èµ°åˆ°äº†critical pointï¼Œgradientè¶¨è¿‘æ–¼0ï¼Œä½†æ˜¯gradientçœŸçš„è®Šå¾—å¾ˆå°äº†å—?å¾ä¸Šé¢çš„åœ–ç‰‡å¯ä»¥çœ‹å‡ºï¼Œ==åœ¨lossæ¸›å°‘çš„æ›²ç·šé€™éº¼å¹³ç·©çš„æ™‚å€™ï¼Œgradientå…¶å¯¦ä¸¦æ²’æœ‰å¾ˆå°==ï¼Œç”šè‡³åˆ°æœ€å¾Œé‚„æœ‰çªç„¶è®Šå¤šä¸€é»ï¼Œé‚£ç‚ºä»€éº¼lossé²é²ç„¡æ³•é™ä½å‘¢?å…¶ä¸­ä¸€å€‹å¯è¦–åŒ–çš„ä¾‹å­å°±æ˜¯å·¦ä¸Šåœ–ï¼Œåœ¨ä¸€å€‹å³½è°·ä¸­é–“åè¦†æ©«è·³ï¼Œé€²ä¸å»å³½è°·è£¡é¢ã€‚è€Œé€šå¸¸åœ¨çœŸçš„èµ°åˆ°critical pointä¹‹å‰ï¼Œlosså°±æœƒåƒé€™æ¨£å¡ä½äº† </br></br>

:::success
âœ‹**ä¿®ä½†å¹¾å’§** </br>
Q: æ—¢ç„¶èªªlosså¡ä½å…¶å¯¦å¾ˆå°‘æ˜¯å› ç‚ºå¡åœ¨critical pointï¼Œé‚£éº¼å‰é¢çš„é€™å¼µåœ–æ˜¯æ€éº¼ä¸€å›æ˜¯?
![](https://i.imgur.com/HDet9Ye.png) </br></br>
*A: å…¶å¯¦è¦è¨“ç·´åƒæ•¸åˆ°å¾ˆæ¥è¿‘critical pointï¼Œç”¨ä¸€èˆ¬çš„gradient descentæ˜¯åšä¸åˆ°çš„ï¼Œè¦å‡ºç¾é€™æ¨£åœ–è¦æœ‰ç‰¹åˆ¥çš„æ–¹æ³•ï¼Œæ­£å› ç‚ºå¦‚æ­¤ï¼Œå¹³å¸¸æˆ‘å€‘è¦é‡åˆ°critical pointæ˜¯å¾ˆå›°é›£çš„*
:::

ä¾†çœ‹çœ‹ä¸‹é¢é€™å¼µerror surfaceï¼Œé€™å€‹æ©¢åœ“å½¢çš„error surfaceåœ¨ç¸±è»¸çš„åœ°æ–¹å¡åº¦éå¸¸é™¡å³­ï¼Œæ©«è»¸çš„åœ°æ–¹å‰‡å¡åº¦éå¸¸å¹³ç·©ï¼Œé›–ç„¶é€™æ˜¯å€‹æ§‹é€ å–®ç´”çš„error surfaceï¼Œä½†gradient descentå»ä¸è¦‹å¾—å¯ä»¥æŠŠå®ƒåšå¥½
![](https://i.imgur.com/qOnSjGw.png) </br></br>

å…ˆä¾†çœ‹çœ‹ $\eta = 10^{-2}$ çš„ç‹€æ³: </br>
![](https://i.imgur.com/kOxz1Jk.png) </br>
åƒæ•¸åœ¨å±±åœ°çš„å…©ç«¯ä¸æ–·éœ‡ç›ªæ‰ä¸ä¸‹å»ï¼Œé€™æ™‚ä½ å¯èƒ½æœƒè¦ºå¾—ï¼Œé€™æ˜¯ $\eta$ è¨­å®šå¤ªå¤§çš„ç·£æ•…ï¼Œåªè¦èª¿ä½learning rateå°±å¥½äº†å•Š! </br></br>

æ–¼æ˜¯ä¸åœåœ°çš„èª¿ä½è‡³ $\eta = 10^{-7}$ ç›´åˆ°åƒæ•¸ä¸å†éœ‡ç›ª:
![](https://i.imgur.com/B5GRujJ.png) </br>
ä½†é€™æ™‚å•é¡Œåˆä¾†äº†ï¼Œç•¶å¡åº¦éå¸¸å¹³æ»‘çš„æ™‚å€™ï¼Œé€™éº¼å°çš„learning rateæ ¹æœ¬ç„¡æ³•é †åˆ©åœ°èµ°åˆ°ç›®æ¨™

:::info
ğŸ’¡å›ºå®šçš„learning rateé¡¯ç„¶æ˜¯ä¸å¤ å¥½çš„ï¼Œlearning rateæ‡‰è©²è¦ç‚ºæ¯å€‹åƒæ•¸åšå®¢è£½åŒ–!
:::

### å®¢è£½åŒ– Learning Rate

ä¸»è¦çš„å¤§æ–¹å‘å°±æ˜¯ï¼Œåœ¨æ¯”è¼ƒé™¡å³­çš„åœ°æ–¹æˆ‘å€‘å¸Œæœ›learning rateå°ä¸€é»ï¼Œåœ¨å¹³ç·©çš„åœ°æ–¹æˆ‘å€‘å¸Œæœ›learning rateå¤§ä¸€é»ï¼Œæ‰€ä»¥æˆ‘å€‘è¦æ”¹é€ ä¸€ä¸‹ç®—å¼...
![](https://i.imgur.com/WK7Kevb.png)
å°‡ $\eta$ æ”¹æˆ $\frac{\eta}{\sigma_i^t}$ è®“learning rateå¯ä»¥ç‚ºæ¯å€‹åƒæ•¸åšå®¢è£½åŒ–

é‚£å¦‚ä½•æ±‚é€™å€‹ $\sigma_i^t$ å‘¢?å¸¸è¦‹çš„æ–¹æ³•æ˜¯ç®—gradientçš„**Root Mean Square**:
![](https://i.imgur.com/WMxn0CQ.png)
![](https://i.imgur.com/dRpXVoT.png) </br></br>
å¯ä»¥çœ‹åˆ°é€™æ¨£çš„ç­–ç•¥æ˜¯æœ‰æ•ˆçš„ï¼Œç•¶æ•´é«”çš„å¡åº¦å¾ˆå¹³ç·©çš„æ™‚å€™ï¼Œ$\sigma_i^t$ å°±å°ï¼Œ$\frac{\eta}{\sigma_i^t}$ å°±å¤§ï¼Œå¡åº¦å¾ˆé™¡å³­çš„æ™‚å€™å‰‡ç›¸å </br></br>

ä½†ä¸Šé¢çš„æ–¹æ³•é‚„ä¸å¤ å¥½ï¼Œlearning rateæ‡‰è©²é‚„è¦å¯ä»¥éš¨è‘—æ™‚é–“èª¿æ•´ï¼Œä¸‹é¢çš„åœ–å¯ä»¥çœ‹åˆ°ï¼Œé›–ç„¶æ˜¯åŒä¸€å€‹æ–¹å‘ï¼Œä½†<font color = green>ç¶ è‰²ç®­é ­</font>çš„åœ°æ–¹å»æ¯”è¼ƒé™¡å³­ï¼Œéœ€è¦å°çš„learning rateï¼Œè€Œ<font color = red>ç´…è‰²ç®­é ­</font>çš„åœ°æ–¹æ¯”è¼ƒå¹³ç·©ï¼Œéœ€è¦æ¯”è¼ƒå¤§çš„learning rate
![](https://i.imgur.com/01NtqEv.png) </br></br>

æ–¼æ˜¯ä¹å°±æœ‰äº†**RMSProp**é€™å€‹æ–¹æ³•:
![](https://i.imgur.com/tWhDamD.png) </br></br>
ç°¡å–®ä¾†èªªï¼Œç›¸æ¯”åŸå…ˆçš„ç®—å¼ï¼Œå¤šäº†ä¸€å€‹ **$\alpha$** ä¾†æ±ºå®šç¾åœ¨çš„gradientè·Ÿä¹‹å‰å…¶ä»–çš„gradientçš„æ¬Šé‡ </br></br>
![](https://i.imgur.com/FFRFuFz.png) </br></br>

è€Œæˆ‘å€‘å¸¸ç”¨çš„optimizationç­–ç•¥å°±æ˜¯**Adam**:
![](https://i.imgur.com/WUK34X3.png)
:::info
â„¹ï¸[AdamåŸå§‹è«–æ–‡](https://arxiv.org/pdf/1412.6980.pdf)
:::
å›åˆ°åŸæœ¬çš„é€™å¼µåœ–ï¼Œæœ‰äº†èƒ½å¤ è‡ªå‹•èª¿æ•´çš„learning rateä¹‹å¾Œï¼Œå°±æˆåŠŸtrainèµ·ä¾†äº†ï¼Œè‡³æ–¼<font color = red>ç´…è‰²åœˆåœˆ</font>çš„éƒ¨åˆ†ï¼Œæ˜¯å› ç‚ºèµ°äº†å¾ˆé•·ä¸€æ®µè·¯ä¹‹å¾Œï¼Œç´¯ç©äº†å¾ˆå¤šå¾ˆå°çš„gradientï¼Œå¹³å‡ä¸‹ä¾†å°±å°äº†ï¼Œæ‰€ä»¥learning rateåˆè®Šå¤§ç›´æ¥é–‹å™´ï¼Œä½†ä¸ç”¨æ“”å¿ƒï¼Œå™´äº†å¹¾å€‹å¾ˆå¤§çš„gradientä¹‹å¾Œï¼Œlearning rateåˆæœƒè®Šå°
![](https://i.imgur.com/CaEJw7p.png) </br></br>

**Q: é‚£è¦å¦‚ä½•é¿å…é€™æ¨£çš„ç‹€æ³å‘¢?** </br>
*A: Learning Rate Scheduling!!* </br></br>
éš¨è‘—ä¸æ–·çš„updateæˆ‘å€‘é›¢çµ‚é»è¶Šä¾†è¶Šè¿‘ï¼Œæ–¼æ˜¯æˆ‘å€‘æŠŠlearning rateæ¸›å°ï¼Œè§£æ±ºäº†åˆ°å¾Œé¢æœƒäº‚å™´çš„å•é¡Œ
![](https://i.imgur.com/4qQuVYc.png) </br></br>

Learning Rate Schedulingä¹Ÿä¸æ­¢æœ‰Learning Rate Decayé€™å€‹æ–¹æ³•ï¼Œé‚„æœ‰æ¯”å¦‚**Warm Up**é€™å€‹æ–¹æ³•:
![](https://i.imgur.com/eHwOwVU.png) </br></br>
é€™å€‹æ–¹æ³•åœ¨è¨“ç·´ä¹‹å¾Œæœƒè¬›çš„**BERT**ä¸­å¸¸å¸¸çœ‹åˆ°ï¼Œä½†ä¸æ˜¯å…ˆæœ‰**BERT**æ‰æœ‰**Warm Up**çš„ã€‚è‡³æ–¼ç‚ºä»€éº¼Warm Upé€™å€‹æ–¹æ³•æœƒæœ‰æ•ˆï¼Œé›–ç„¶é‚„æ²’æœ‰ä¸€å€‹ç¢ºåˆ‡çš„ç­”æ¡ˆï¼Œä½†å¯ä»¥é€™æ¨£è§£é‡‹: ==æ—¢ç„¶æˆ‘å€‘çš„learning rateæ˜¯é€éä¹‹å‰çš„gradientç¸½å’Œä¾†é€²è¡Œèª¿æ•´çš„ï¼Œåœ¨å‰›èµ·æ­¥çš„æ™‚å€™ï¼Œé€™å€‹çµ±è¨ˆæ•¸æ“šçš„è³‡æ–™é‚„ä¸å¤ å¤š==ï¼Œæ‰€ä»¥ä¸€é–‹å§‹å…ˆå£“ä½learning rateï¼Œç›´åˆ°è³‡æ–™å¤ å¤šçš„æ™‚å€™ï¼Œæ‰æé«˜learning rate
:::info
â„¹ï¸å»¶ä¼¸é–±è®€: [RAdam](https://arxiv.org/abs/1908.03265)
:::

**ç¸½çµä¸€ä¸‹:** </br>
![](https://i.imgur.com/9yOHgRN.png) </br></br>

å­¸äº†é€²éšçš„optimizationæ–¹æ³•ä¹‹å¾Œï¼Œä½¿å¾—æˆ‘å€‘å¯ä»¥æ‡‰ä»˜å´å¶‡çš„error surfaceï¼Œä½†æœ‰æ²’æœ‰å¯èƒ½å¯ä»¥æŠŠerror surfaceç›´æ¥ç‚¸å¹³å‘¢?å¦‚æœerror surfaceè®Šå¾—ä¸é‚£éº¼å´å¶‡ï¼Œå°±èƒ½æ›´åˆ©æ–¼training!
![](https://i.imgur.com/joqZoUV.png)

## é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ (å››)ï¼šæå¤±å‡½æ•¸ (Loss) ä¹Ÿå¯èƒ½æœ‰å½±éŸ¿

### Classification v.s. Regression

ä¸€èˆ¬æˆ‘å€‘è¨“ç·´modelï¼Œæ˜¯è¼¸å…¥ä¸€å€‹ $x$ å¾—åˆ° $y$ ï¼Œé‚£å¯ä»¥åˆ©ç”¨é€™æ¨£çš„æ–¹å¼å»åšåˆ†é¡å—? é€™ç¨®æ–¹æ³•å¯èƒ½æœƒæœ‰ç‘•ç–µï¼Œå‡å¦‚1ä»£è¡¨class 1ï¼Œ2ä»£è¡¨class 2ï¼Œ3ä»£è¡¨class 3ï¼Œé‚£éº¼é€™å°±æ˜¯åœ¨æš—ç¤ºclass 1è·Ÿclass 2æ¯”è¼ƒåƒï¼Œè€Œè·Ÿclass 3æ¯”è¼ƒä¸åƒï¼Œå¦‚æœä»Šå¤©åˆ†é¡çš„æ±è¥¿æœ‰é€™æ¨£çš„é—œä¿‚ï¼Œé‚£æˆ–è¨±é‚„èªªå¾—éå»ï¼Œä½†é€šå¸¸ä¸æœƒæœ‰é€™æ¨£çš„é—œä¿‚ã€‚

![](https://i.imgur.com/QP8FLaG.png)

è¦å¦‚ä½•è§£æ±ºä¸Šè¿°çš„å•é¡Œå‘¢? æˆ‘å€‘å¯ä»¥é€é **one-hot vector** ä¾†è¡¨ç¤ºæ¯ä¸€å€‹class:

![](https://i.imgur.com/TPLeGXa.png)

æ¥è‘—æˆ‘å€‘æ”¹é€ ä¸€ä¸‹modelçš„è¼¸å‡º:

![](https://i.imgur.com/QZTGolH.png)

æ¯”è¼ƒä¸€ä¸‹å…©è€…çš„å·®ç•°:

![](https://i.imgur.com/0RHTt3W.png)

åŸæœ¬è¼¸å‡ºçš„ $y$ æ˜¯ä¸€å€‹æ•¸å€¼ï¼Œè€Œåˆ†é¡è¼¸å‡ºçš„ $y$ å‰‡æ˜¯ä¸€å€‹å‘é‡

:::info
â„¹ï¸é€™è£¡è¦æ³¨æ„çš„æ˜¯ï¼Œmodelå‡ºä¾†çš„ $y$ é‚„è¦ç¶“éä¸€å€‹ $softmax$ å‡½æ•¸ï¼Œæ¯”è¼ƒç°¡å–®çš„èªªæ³•æ˜¯ $\hat{y}$ è£¡é¢çš„å…§å®¹ä¸æ˜¯0å°±æ˜¯1ï¼Œæ‰€ä»¥ $y$ è¦ç¶“éä¸€å€‹æ¨™æº–åŒ–è®Šæˆ $y'$ å†ä¾†å»è·Ÿ $\hat{y}$ ç®—è·é›¢
:::

ç°¡å–®èªªæ˜ä¸€ä¸‹ $softmax$ çš„é‹ä½œ:

![](https://i.imgur.com/nEMtlIf.png)

å–expæ˜¯ç‚ºäº†å¼·åˆ¶è®“è³‡æ–™å…¨è®Šç‚ºæ­£æ•¸ï¼Œä¹‹å¾Œå†åšnormalizeï¼Œé€™æ¨£ä¸€ä¾†æ‰€æœ‰çš„ $y'_i$ å°±æœƒä»‹æ–¼0 ~ 1ä¹‹é–“ï¼Œä¸¦ä¸”ç¸½å’Œç‚º1ã€‚ç¶“é $softmax$ å‡½æ•¸ä¹‹å¾Œï¼Œæœƒè®“å¤§çš„å€¼è·Ÿå°çš„å€¼å·®è·æ›´å¤§ *(ä½ å¯èƒ½æœƒè¦ºå¾—-3è·Ÿ3æ˜æ˜å·®6ï¼Œè¶¨è¿‘æ–¼0è·Ÿ0.88åªå·®å¤§ç´„0.88å“ªæœ‰æ¯”è¼ƒå¤šï¼Ÿä½†æ˜¯åŸæœ¬çš„ä¸Šä¸‹é™æ˜¯-âˆ ~ âˆå–”ï¼Œnormalizeä¹‹å¾Œçš„ä¸Šä¸‹é™æ˜¯0 ~ 1ï¼Œé€™æ¨£æ‡‰è©²æœ‰æ„Ÿè¦ºè¶¨è¿‘æ–¼0è·Ÿ0.88æœ‰å·®æ¯”è¼ƒå¤šäº†å§)*

:::info
â„¹ï¸å¦‚æœæ˜¯åªæœ‰2å€‹classè¦é€²è¡Œåˆ†é¡çš„è©±ï¼Œé›–ç„¶ä¹Ÿæ˜¯å¯ä»¥ç”¨ $softmax$ï¼Œä½†æœƒæ›´å¸¸çœ‹åˆ°æœ‰äººç›´æ¥ç”¨ $sigmoid$ï¼Œé€™ç¨®æƒ…æ³ä¸‹é€™å…©å€‹å‡½æ•¸å‡ºä¾†çš„çµæœæ˜¯ç›¸åŒçš„
:::

### Lost of Classification

è¦è¨ˆç®— $y'$ å’Œ $\hat{y}$ ä¹‹é–“çš„è·é›¢ï¼Œæœ‰ä¸åªä¸€ç¨®æ–¹æ³•ï¼Œä½†æ¯”èµ·æˆ‘å€‘ç†Ÿæ‚‰çš„MSEï¼Œæ›´ç‚ºå¸¸è¦‹çš„ä½œæ³•æ˜¯ **Cross-entropy**ï¼Œç•¶ $y'$ å’Œ $\hat{y}$ ä¸€æ¨£çš„æ™‚å€™ï¼Œæˆ‘å€‘ä¸€æ¨£å¯ä»¥minimize cross-entropy çš„å€¼

![](https://i.imgur.com/yLCKKho.png)

ä»¥çµæœä¾†èªªï¼Œ**Cross-entropy** æ¯”èµ·MSEï¼Œæ›´é©åˆç”¨åœ¨åˆ†é¡ä¸Šï¼Œé€™å€‹å‡½æ•¸å¸¸ç”¨åˆ°å®ƒåœ¨pytorchä¸­ï¼Œæ˜¯ç›´æ¥è·Ÿ **Soft-max** ç¶å®šåœ¨ä¸€èµ·çš„ï¼Œåªè¦å‘¼å« **Cross-emtropy** è£¡é¢å°±æœƒè‡ªå‹•å…§å»º **Soft-max** å‡½æ•¸åœ¨æœ€å¾Œä¸€å±¤ï¼Œæ‰€ä»¥å¦‚æœä½ ç”¨ **Cross-entropy** çš„æ™‚å€™åˆè‡ªå·±åŠ äº† **Soft-max**ï¼Œå°±æœƒè®Šæˆç¶“éå…©å±¤çš„ **Soft-max**

![](https://i.imgur.com/iwRRyL0.png)

é€™è£¡ç°¡å–®èªªæ˜ä¸€ä¸‹ç‚ºä½• **Cross-entropy** æ›´å¥½ï¼Œç•¶losså°çš„æ™‚å€™ MSE å’Œ Cross-entropy æ˜¯å·®ä¸å¤šçš„ï¼Œä½†æ˜¯ç•¶losså¤§çš„æ™‚å€™ï¼ŒCross-entropy æ˜¯æœ‰æ–œç‡çš„ï¼Œè€Œ MSE å»éå¸¸å¹³å¦ï¼Œé€™æœƒé€ æˆæˆ‘å€‘åœ¨åš gradient desent çš„æ™‚å€™çª’ç¤™é›£è¡Œã€‚ç•¶ç„¶ï¼Œä¸Šé¢çš„ä¾‹å­æ˜¯å»ºç«‹åœ¨æˆ‘å€‘ä½¿ç”¨äº†ä¸€å€‹ä¸æ€éº¼æ¨£çš„optimizerçš„ç‹€æ³ä¸‹ï¼Œå‡å¦‚æˆ‘å€‘ç”¨ Adam ï¼Œåœ¨gradientå¾ˆå°çš„æ™‚å€™è‡ªå‹•èª¿å¤§ learning rateï¼Œé‚£æˆ–è¨±é‚„æ˜¯å¯ä»¥èµ°åˆ° loss å°çš„åœ°æ–¹ï¼Œä½†æ¯”èµ·ä½¿ç”¨ Cross-entropyï¼ŒMSE ç¢ºå¯¦æœƒè®“è¨“ç·´è®Šå¾—å›°é›£ä¸€äº›ï¼Œæ‰€ä»¥æå¤±å‡½æ•¸å°æ–¼ optimization æ˜¯ç¢ºå¯¦æœ‰å½±éŸ¿çš„ï¼

[ç¹¼çºŒé–±è®€â¡ï¸]()
